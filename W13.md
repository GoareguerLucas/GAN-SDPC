# Résumé de la semaine 13 de stage


## A faire :

- [x]  Mettre à jour l'ordinateur
- [x]  Trouvé un dataset Simpsons
- [x]  Construire un module Dataset torch
- [x]  Nettoyer le dataset Dataset (cf. W10_dataset_dcgan)
- [x]  Trouver un GAN efficace sur le dataset
- [ ] Tunner le GAN jusqu'à avoir un résultats concluant
- [ ] Tester CycleGAN pour transformer des visages humain en Simpsons
- [ ] Prendre en main SDPC
- [ ] Trouver une architecture pour fusionner le GAN et SDPC
- [ ] Evaluer l'intèret de l'architecture
- [ ] Tester BigGan qui promet de bien marcher mais demande beaucoup de ressource : Peut être
- [x] from skimage.color import rgb2hsv
- [x] https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomAffine
- [x] Apprendre à utliser Tensorboard
- [x] Ajout d'un module de scan des paramètre compatible avec tensorboard (cf. current pour plus de détails)
- [x] Ajouter des affichages de résultats plus complets (std, coefficient de variation,..)

## Mise à jour des codes communs

  - Ajout d'une fonction scan dans utils.py qui permet de gérer facilement le scan des paramètres (cf. current pour plus de détails).
  - Les fonctions de plots qui était dans utils.py on était déplacer vers plot.py.

###Utilisation de tensorboard :

1 - Dans le code [Tutoriel](https://www.tensorflow.org/guide/summaries_and_tensorboard) [Doc](https://pytorch.org/docs/stable/tensorboard.html)
2 - Les codes compatibles sauvegardes les données dans un dossier, par défault ./runs/.
3 - Puis la commande : tensorboard --logdir=runs, permet de lancer un serveur pour visualiser ces données durant l'entraînement sur le port 6006.
4 - Connexion sur le port 6006 du GPU depuis le port 16006 de l'ordinateur : ssh -p8012 -L16006:localhost:6006 g14006889@gt-0.luminy.univ-amu.fr
5 - Ouverture d'un navigateur à l'adresse : http://localhost:16006

## Note d'expériences

#### Test Current en 64x64 batchsize=64 epochs=200 
Nouvelles base avec passage complet à tensorboard et ajout d'un scan des paramètres.

L'usage du scan des paramètres à était rendu le plus simple possible.
Le code ce trouve dans la fonction scan de utils.py.
Il est nécessaires de lire W13_current_dcgan/scan_params.py pour bien comprendre l'usage.

__Résultats__ :
  - DCGAN : Cette version est un test pour développer de nouvelles technique, les résultats ne sont pas commenter 
    Time=
		
__Conclusion__ :
  - Tensorboard est maintenant l'outil unique de visualisation des résultats.
  - L'outil de scan des paramètres fonctionne correctement et est compatible avec tensorboard.
  - Cette version est une base d'outil et d'usage qui devrait accélérer le paramétrage des modèles.
  - VERSION COURANTE 
  
#### Test lr-esp en 64x64 batchsize=64 epochs=200 
Test grandeur nature du scan des paramètres
Scan des paramètre :
  - lrG
  - lrD
  - eps : Batch normalisation
cf. scan_params.py

__Résultats__ :
  - lrG : Si lrG est trop haut G apprend trop vite et D stop rapidement sont apprentissage donc G ne peut plus apprendre et tout s'arrête.
    Time=1h15m (x4)
  - lrD : Avec un lrD plus faible le lossD continue de diminuer tout le long, le lossG augmente moins haut et moins vite et les scores sont plus proche de 0.5 et s'en éloignent moins vite.
    Time=1h15m (x4)
  - eps : Avec un valeur de 0.5 l'apprentissage est plus rapide et les images "semble" meilleur que sans BN.
    Time=1h15m (x4)
		
__Conclusion__ :
  - Bug : Les images enregistrer n'ont pas était normaliser avant l'enregistrement, elle ne correspondes pas exactement à la réalité.
  - Les commentaires sur les images sont sujets au bug de normalisation
  - lrG : Trop haut G devient trop bon pour D. Meilleurs valeurs tester : 0.0001. 
  - lrD : Choisir une valeur faible. Meilleurs valeurs tester : 0.00001.
  - eps : Bénéfice certain. Apprentissage accélérer. Meilleurs valeurs tester : 0.5. 
  - Tensorboard est vraiment efficace, facile à prendre en mains et efficace pour comparer de nombreux résultats. 
  - Les images ne semble pas toutes enregistrer, pourquoi ?

#### Test latentDim en 64x64 batchsize=64 epochs=200 
Exploration des paramètres 
Scan des paramètre :
  - latent_dim : [50, 150, 200], on pourras regarder runs/Lr_Esp/lrG0.0001lrD1e05eps0.5/ pour une version avec latent_dim==100
cf. scan_params.py

__Résultats__ :
  - latent_dim : 
    Time= ()
		
__Conclusion__ :
  - :
  
#### Test batchsize en 64x64 epochs=200 
Exploration des paramètres 
Scan des paramètre :
  - batch_size : [8, 16, 32], on pourras regarder runs/Lr_Esp/lrG0.0001lrD1e05eps0.5/ pour une version avec batch_size==64
cf. scan_params.py

__Résultats__ :
  - batch_size : 
    Time= ()
		
__Conclusion__ :
  - :
