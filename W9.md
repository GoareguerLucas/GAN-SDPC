# Résumé de la semaine 9 de stage


## A faire :

- [x]  Mettre à jour l'ordinateur
- [x]  Trouvé un dataset Simpsons
- [x]  Construire un module Dataset torch
- [x]  Trouver un GAN efficace sur le dataset
- [ ] Tunner le GAN jusqu'à avoir un résultats concluant
- [ ] Tester CycleGAN pour transformer des visages humain en Simpsons
- [ ] Prendre en main SDPC
- [ ] Trouver une architecture pour fusionner le GAN et SDPC
- [ ] Evaluer l'intèret de l'architecture
- [ ] Tester BigGan qui promet de bien marcher mais demande beaucoup de ressource : Peut être
- [ ] from skimage.color import rgb2hsv
- [ ] https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomAffine

## Note d'expériences

#### Test PerfectG en 64x64 batchsize=64 epochs=300 
Que ce passe t-il si G devient presque parfait ?
Pour simuler un G parfait, à un moment de l'entraînement on remplace les images générer par des images du dataset avec un bruit normal (G(z) devient x+noise).
Hypothèse : Comme D après quelque epochs est devenus très bon on peut s'attendre à ce qu'il deviennent incapable de différencier x et x+noise (ou moins dans un premier temps). 

__Résultats__ :
  - DCGAN (gt-2 dcgan): 
		Time=
		
__Conclusion__ :
  -

#### Test Long en 64x64 batchsize=64 epochs=5000 
Le lossG reste t-il stable après beaucoup d'epochs
HRF et noise (pour D(x) et D(G(z))) pour améliorer la stabilité 
Hypothèse : Le lossG converge vers un nombre arbitraire (qui n'est pas 0) alors que lossD converge vers 0

__Résultats__ :
  - DCGAN (gt-2 dcgan2):
		Time=
		
__Conclusion__ :
  - 
