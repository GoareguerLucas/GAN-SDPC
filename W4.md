# Résumé de la semaine 4 de stage


## A faire :

- [x]  Mettre à jour l'ordinateur
- [x]  Trouvé un dataset Simpsons
- [x]  Construire un module Dataset torch
- [x]  Trouver un GAN efficace sur le dataset
- [ ] Tunner le GAN jusqu'à avoir un résultats concluant
- [ ] Tester CycleGAN pour transformer des visages humain en Simpsons
- [ ] Prendre en main SDPC
- [ ] Trouver une architecture pour fusionner le GAN et SDPC
- [ ] Evaluer l'intèret de l'architecture
- [ ] Tester BigGan qui promet de bien marcher mais demande beaucoup de ressource : Peut être

## Note d'expériences

##### Test Current en 32*32 epochs=5000
Teste de verification des modification et ajout de la phase d'optimisation, gt-2.

__Résultats__ :
  - GAN : Tout c'est bien passer.
  - DCGAN : Tout c'est bien passer.
  - AAE : Tout c'est bien passer.

__Conclusion__ :
  - DCGAN : Version fonctionnel ! 
  - GAN : Version fonctionnel ! 
  - AAE : Version fonctionnel !
  - Ces version sont à utiliser comme base de travail pour les futures expériences.
  
##### Test DCGANlong35k en 32*32 lr=0.0004 epochs=35000
test long avec LabelSmooth uniform (0.9-1.0)

__Résultats__ :
  - DCGAN : Difficile de dire que l'apprentissage c'est poursuivie au délà de 5k epochs néanmoins les images générer sont, à la fin, un peut meilleur qu'avec DCGANlong5k.
		Les images sont souvent bonnes, on reconnai les personnages et ils on presque tous le bon nombre d'yeux.
		
__Conclusion__ :
  - DCGAN : Les courbes montrent que l'apprentissage est bloquer mais les résultats sont prometteur. Augmenter plus de nombre epochs n'aidera pas (ou trop peut).

##### Test BEGAN en 32*32 epochs=5000 lr=0.00004 gamma=0.5
Prise en compte d'un certain nombre de commentaire trouver sûre git et augmentation du nombre d'epochs 

__Résultats__ :
  - BEGAN : On constate que les losses et M sont complètement bloquer, le modèle n'apprent plus. Les images générer sont toutes les mêmes, mode collapse.
		Time=11h30

__Conclusion__ :
  - Il faut t'enter de réduire le learning rate lorsque M ne diminue plus.
  - Les paramètres choisi pour W3_current_BEGAN donner de meilleurs images (pas de mode collapse)

##### Test BS en 32*32 epochs=5000 BatchSize = 32
Tentative de réduction du BatchSize pour éviter les minimums locaux [ref](https://github.com/carpedm20/BEGAN-tensorflow/issues/42) 

Comparer avec W4_current_*

__Résultats__ :
  - GAN : De meilleurs images que précèdement. Sur une même iteration les samples générer sont peut diverssifier (mode collapse). Les courbes ne présente pas de difference importante.
		Time=9h50
  - DCGAN : Des images et des courbes très similaires.
		Time=13h30
  - AAE : Des résultats très similaires au précédents.
		Time=12h30

__Conclusion__ :
  - Dans l'ensemble les trois courbes sont plus marquer et montrent que D prend l'avantage plus vite.
  - Les résultats sont vraiment proche de ceux obtenues avec un BatchSize à 256
  - Temps de calcul nettement plus long.

##### Test decay en 32*32 epochs=1000 lr=0.0004 gamma=0.5
Ajout d'un scheduler pour gerrer le lr.

__Résultats__ :
  - BEGAN (gt-0): 
		Time=

__Conclusion__ :
  - :

##### Test b1 en 32*32 epochs=5000 b1=0.9 
Passage du paramètre beta1 de Adam à 0.9 (paramètre par défault) au lieux de 0.5 [ref:3.4](https://arxiv.org/pdf/1703.10717.pdf) 
LabelSmooth uniform à (0.9-1.0)

Comparer avec W4_current_*, mais LabelSmooth different et BatchSize = 256 !

__Résultats__ :
  - GAN : 
		Time=
  - DCGAN : 
		Time=
  - AAE : 
		Time=

__Conclusion__ :
  - :
