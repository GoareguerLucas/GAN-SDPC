# Résumé de la semaine 4 de stage


## A faire :

- [x]  Mettre à jour l'ordinateur
- [x]  Trouvé un dataset Simpsons
- [x]  Construire un module Dataset torch
- [x]  Trouver un GAN efficace sur le dataset
- [ ] Tunner le GAN jusqu'à avoir un résultats concluant
- [ ] Tester CycleGAN pour transformer des visages humain en Simpsons
- [ ] Prendre en main SDPC
- [ ] Trouver une architecture pour fusionner le GAN et SDPC
- [ ] Evaluer l'intèret de l'architecture
- [ ] Tester BigGan qui promet de bien marcher mais demande beaucoup de ressource : Peut être

## Note d'expériences

##### Test Current en 32*32 epochs=5000 BatchSize=256 b1=0.5
Teste de verification des modification et ajout de la phase d'optimisation, gt-2.

__Résultats__ :
  - GAN : Tout c'est bien passer.
  - DCGAN : Tout c'est bien passer.
  - AAE : Tout c'est bien passer.

__Conclusion__ :
  - DCGAN : Version fonctionnel ! 
  - GAN : Version fonctionnel ! 
  - AAE : Version fonctionnel !
  - Ces version sont à utiliser comme base de travail pour les futures expériences.
  
##### Test DCGANlong35k en 32*32 lr=0.0004 epochs=35000
test long avec LabelSmooth uniform (0.9-1.0)

__Résultats__ :
  - DCGAN : Difficile de dire que l'apprentissage c'est poursuivie au délà de 5k epochs néanmoins les images générer sont, à la fin, un peut meilleur qu'avec DCGANlong5k.
		Les images sont souvent bonnes, on reconnai les personnages et ils on presque tous le bon nombre d'yeux.
		
__Conclusion__ :
  - DCGAN : Les courbes montrent que l'apprentissage est bloquer mais les résultats sont prometteur. Augmenter plus de nombre epochs n'aidera pas (ou trop peut).

##### Test current_BEGAN en 32*32 epochs=5000 lr=0.00004 gamma=0.5
Prise en compte d'un certain nombre de commentaire trouver sûre git et augmentation du nombre d'epochs 

__Résultats__ :
  - BEGAN : On constate que les losses et M sont complètement bloquer, le modèle n'apprent plus. Les images générer sont toutes les mêmes, mode collapse.
		Time=11h30

__Conclusion__ :
  - Il faut t'enter de réduire le learning rate lorsque M ne diminue plus.
  - Les paramètres choisi pour W3_current_BEGAN donner de meilleurs images (pas de mode collapse)

##### Test BS en 32*32 epochs=5000 BatchSize = 32
Tentative de réduction du BatchSize pour éviter les minimums locaux [ref](https://github.com/carpedm20/BEGAN-tensorflow/issues/42) 

Comparer avec W4_current_*

__Résultats__ :
  - GAN : De meilleurs images que précèdement. Sur une même iteration les samples générer sont peut diverssifier (mode collapse). Les courbes ne présente pas de difference importante.
		Time=9h50
  - DCGAN : Des images et des courbes très similaires.
		Time=13h30
  - AAE : Des résultats très similaires au précédents.
		Time=12h30

__Conclusion__ :
  - Dans l'ensemble les trois courbes sont plus marquer et montrent que D prend l'avantage plus vite.
  - Les résultats sont vraiment proche de ceux obtenues avec un BatchSize à 256
  - Temps de calcul nettement plus long.



##### Test decay en 32*32 epochs=1000 lr=0.0004 gamma=0.5 BatchSize = 32
Ajout d'un scheduler pour gerrer le lr.

__Résultats__ :
  - BEGAN : Un mode collapse complet, aucune difference dans les images générer.
		Time=2h20

__Conclusion__ :
  - Le lr diminue au début de l'apprentissage sans pour autant avoir d'impacte sur M
  
##### Test decay2 en 32*32 epochs=5000 lr=0.0004 gamma=0.75 BatchSize = 32 b1=0.9
Modification des hyper-paramètres

Comparer avec W4_decay_began

__Résultats__ :
  - BEGAN : Plus de mode collapse. Des courbes rapidement stagnantes à des valeurs similaires à W4_decay_began.
		Time=11h30

__Conclusion__ :
  - Gamma est un paramètre important qu'il va falloir régler.
  - L'impacte de la diminution du lr n'est pas visible.



##### Test b1 en 32*32 epochs=5000 b1=0.9 
Passage du paramètre beta1 de Adam à 0.9 (paramètre par défault) au lieux de 0.5 [ref:3.4](https://arxiv.org/pdf/1703.10717.pdf) 
LabelSmooth uniform à (0.9-1.0)

Comparer avec W4_current_*, mais LabelSmooth different !

__Résultats__ :
  - GAN : Un loss de G qui part de très haut avant de ce stabiliser à 3. 
		Time=2h30
  - DCGAN : Des images moins nette mais plus de personnage reconnaissable. L'apprentissage stop comme d'abitude.
		Time=7h15
  - AAE : Il semble que l'apprentissage ce stop un peut plus vite. Image très semblable.
		Time=3h

__Conclusion__ :
  - Le changement de LabelSmoothing à peut être perturber les résults.
  - Peut être qu'un b1 plus faible pourrait aider à maintenir l'arpprentissage plus longtemps.
  - Moins d'epochs utile pour constater un apprentissage (1000 max)

##### Test b1-0.1 et b1-0.9 en 32*32 epochs=1000 b1=0.1-0.9 
Etude de l'impact du paramètre b1 (Adam) sur l'apprentissage.
LabelSmooth uniform à (0.7-1.0)

Comparer avec W4_current_* et W4_b1-0.1_* et W4_b1-0.9_*

__Résultats__ b1=0.1:
  - GAN (gt-2): Les meilleurs images pour un GAN. Les courbes semblent montrer que D prend très tôt l'avantage.
		Time=25m
  - DCGAN (gt-2): Image flou et visage peut marquer
		Time=1h25
  - AAE (gt-2): Les courbes semblent montrer un apprentissage plus divergeant qu'avec b1=0.9. Les images ne présente pas de difference.
		Time=35m
		
__Résultats__ b1=0.9:
  - GAN (gt-0): Mode collapse. Bouillie de pixels jaune. 
		Time=30m
  - DCGAN (gt-0): Image plus nette que b1=0.1 et b1=0.5(current).
		Time=1h30
  - AAE (gt-0): Les courbes semblent montrer un apprentissage moins divergeant qu'avec b1=0.1. Les images ne présente pas de difference.
		Time=40m

__Conclusion__ :
  - GAN : Le GAN à profiter d'un b1=0.1 donc faible.
  - DCGAN : Ce model présente une nette amèlioration entre 0.1 et 0.9 et une amèlioration  moins nette entre 0.5 et 0.9. Un b1 encore plus élever pourrait amèliorer les résultats.
  - AAE : La courbe de score semble montrer que les scores D(x) et D(G(z)) diverge moins lorsque b1 est élever. Néanmoins il n'en viennent pas encore à converger


##### Test Gamma en 32*32 epochs=1000 lr=0.0004 gamma=0.9-0.6
Etude de l'impacte du paramètre Gamma sur BEGAN

__Résultats__ :
  - BEGAN Gamma=0.9: Des images diversses. Un apprentissage très vite quasi stopper.
		Time=2h15
  - BEGAN Gamma=0.6 (gt-2): Mode collapse. M est complètement plat après très peut d'epochs.
		Time=2h15
		
__Conclusion__ :
  - Je pense que le mode collapse ne dépend pas de gamma. Je relance 0.6 pour voir. En faite 0.6 à de nouveau mode collapse.
  - Au niveau des courbe on ne constate pas de grand changement, l'apprentissage est toujours aussi cours.
  - Le model utiliser n'est pas bon je vais tester d'autres achi plus proche du papier d'origine
  
##### Test BEGANarchi en 32*32 epochs=1000 lr=0.0004 gamma=0.5
Utilisation d'une architecture de BEGAN plus proche de celle du papier d'origine.
Le BEGAN utiliser pour le moment ne corespond pas vraiment à celui présenter dans le papier

__Résultats__ :
  - BEGAN :
		Time=
		
__Conclusion__ :
  - :
