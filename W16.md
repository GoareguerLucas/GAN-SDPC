# Résumé de la semaine 16 de stage


## A faire :

- [x]  Mettre à jour l'ordinateur
- [x]  Trouvé un dataset Simpsons
- [x]  Construire un module Dataset torch
- [x]  Nettoyer le dataset Dataset (cf. W10_dataset_dcgan)
- [x]  Trouver un GAN efficace sur le dataset
- [ ] Tunner le GAN jusqu'à avoir un résultats concluant
- [ ] Tester CycleGAN pour transformer des visages humain en Simpsons
- [ ] Prendre en main SDPC
- [ ] Trouver une architecture pour fusionner le GAN et SDPC
- [ ] Evaluer l'intèret de l'architecture
- [ ] Tester BigGan qui promet de bien marcher mais demande beaucoup de ressource : Peut être
- [x] from skimage.color import rgb2hsv
- [x] https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomAffine
- [x] Apprendre à utliser Tensorboard
- [x] Ajout d'un module de scan des paramètre compatible avec tensorboard (cf. current pour plus de détails)
- [x] Ajouter des affichages de résultats plus complets (std, coefficient de variation,..)
- [x] Création d'un dataset baser sur un espace latent connue [cf. ref](http://datashader.org/topics/strange_attractors.html)

## Mise à jour des codes communs

###Utilisation de tensorboard :

1 - Dans le code [Tutoriel](https://www.tensorflow.org/guide/summaries_and_tensorboard) [Doc](https://pytorch.org/docs/stable/tensorboard.html)
2 - Les codes compatibles sauvegardes les données dans un dossier, par défault ./runs/.
3 - Puis la commande : tensorboard --logdir=runs, permet de lancer un serveur pour visualiser ces données durant l'entraînement sur le port 6006.
4 - Connexion sur le port 6006 du GPU depuis le port 16006 de l'ordinateur : ssh -p8012 -L16006:localhost:6006 g14006889@gt-0.luminy.univ-amu.fr
5 - Ouverture d'un navigateur à l'adresse : http://localhost:16006

## Note d'expériences

#### Test AE_dcgan_large en 32x32 epochs=200
Utilisation de tensorboard pour l'Adverserial Auto-Encoder.
Paramétrages de l'architecture booster.
Objectif : Trouver des paramètres efficace pour ce modèle en 32x32 pour ensuite les tester en 128x128.
Scan des paramètre :
  - eps
cf. scan_params.py
  - b1
cf. scan_params2.py
  - lrD
  - lrG
  - lrE
cf. scan_params3.py : Comparer avec eps=0 et comparer le facteur entre lrG et lrD.
  - lrelu
cf. scan_params4.py (board gt-0)
  - lrG
  - lrE
cf. scan_params5.py (board2 gt-0)

__Résultats__ :
  - dcgan (board[1,2] gt-0): 
    - La Batch Normalization n'as pas un effet bénéfique sur ce modèle. 
    - Un b1=0.9 fonctionne mieux que des valeurs plus basses.
    - lr : 
      - Les valeurs lrG0.001lrE0.001lrD0.0001 rende le modèle instable car elle sont trop élever.
      - Un facteur 10 entre lrD et lrG/lrE est trop élever. Les meilleurs résultats on était obtenus avec un facteur x3.
      - La configuration lrG0.0001lrE0.0001lrD1e05 montrent des courbes encourageante mais des images moins bonne qu'avec un facteur x3.
    Time= 3h25m
		
__Conclusion__ :
  - Meilleur valeur tester :
    - b1 : 0.9
    - eps : 0.0
    - lrD : 0.00005
    - lrG : 0.00015
    - lrE : 0.00015
    - lrelu :
  - lr : Le facteur entre les différents lr semble avoir était trop élever (x10). Il faut utiliser des lr plus proches.
  - Un facteur entre les lr de x3 ou x4 avec un lrD plus bas que les Meilleur valeurs trouvée pourrait être efficace.
    - Après le teste de lrG0.0001lrE0.0001lrD0.000025 (x4 et moins grand que les meilleurs valeurs trouver) le teste n'a pas était concluant.
  
#### Test AE_64_large en 64x64 epochs=200
Utilisation de tensorboard pour l'Adverserial Auto-Encoder.
Paramétrages de l'architecture booster.
Objectif : Vérifier que les paramètres trouver en 32x32 fonctionne avec une taille plus élever.
Paramètre :
  - lrD : 0.00005
  - lrG : 0.00015
  - lrE : 0.00015
  - eps : 0.0
  - b1 : 0.9

__Résultats__ :
  - dcgan (board gt-2): 
    Time= 
		
__Conclusion__ :
  - :
  
#### Test AE_dcgan_boost en 64x64 epochs=200
Utilisation de nombreux filtres et de kernels de taille 9 pour un DCGAN.
Paramétrages de l'architecture booster.
Objectif : Déterminer si une architecture booster permet au modèle DCGAN d'avoir de meilleur résultats.
Scan des paramètre :
  - lrD 
  - lrG 
cf. scan_params.py

__Résultats__ :
  - dcgan : 
    Time= 
		
__Conclusion__ :
  - :
  
#### Test FDD_cgan en 64x64 epochs=200
Mise en pratique du Conditionnal DCGAN sur le datatset FDD
Objectif : Déterminer si le CGAN permet de meilleur résultats 

__Résultats__ :
  - cgan : 
    Time= 
		
__Conclusion__ :
  - :
