# Résumé de la semaine 7 de stage


## A faire :

- [x]  Mettre à jour l'ordinateur
- [x]  Trouvé un dataset Simpsons
- [x]  Construire un module Dataset torch
- [x]  Trouver un GAN efficace sur le dataset
- [ ] Tunner le GAN jusqu'à avoir un résultats concluant
- [ ] Tester CycleGAN pour transformer des visages humain en Simpsons
- [ ] Prendre en main SDPC
- [ ] Trouver une architecture pour fusionner le GAN et SDPC
- [ ] Evaluer l'intèret de l'architecture
- [ ] Tester BigGan qui promet de bien marcher mais demande beaucoup de ressource : Peut être

## Note d'expériences

#### Test 128 en 128x128 batchsize=32 epochs=400 lrG=0.0004 lrD=0.00004 eps=0.00005
Au vus des résultats de W6_128_dcgan il fallait détailler l'évolution des images avant et pendant les pics des courbes de losses.

__Résultats__ :
  - DCGAN (gt-0 dcgan2): 
		Time=12h20
		
__Conclusion__ :
  - 

#### Test 128lr en 128x128 batchsize=32 epochs=400 lrG=0.0001 lrD=0.00001 eps=0.00005
Lr plus faible pour voir l'effet sur les pics de l'expérience W6_128_dcgan
Hypothèse : les pics de l'expériences W6_128_dcgan sont du à un lr trop élever.

__Résultats__ :
  - DCGAN (gt-0 dcgan):
		Time=
		
__Conclusion__ :
  - :

#### Test 128BN en 128x128 batchsize=32 epochs=400 lrG=0.0004 lrD=0.00004 eps=0.1
Une batchNormalization plus élever.
Hypothèse : La batchNormalization c'est le bien.

__Résultats__ :
  - DCGAN (gt-2 dcgan):
		Time=
		
__Conclusion__ :
  - :

#### Test 128Treshold en 128x128 batchsize=32 epochs=400 lrG=0.0004 lrD=0.00004 eps=0.00005
Modification du losses de G.
Les images générer par G qui sont les mieux discriminer par D (D(G(z)) == 0) sont exclue du losses (objectif = 0.5).

__Résultats__ :
  - DCGAN :
		Time=
		
__Conclusion__ :
  - :
