# Résumé de la semaine 7 de stage


## A faire :

- [x]  Mettre à jour l'ordinateur
- [x]  Trouvé un dataset Simpsons
- [x]  Construire un module Dataset torch
- [x]  Trouver un GAN efficace sur le dataset
- [ ] Tunner le GAN jusqu'à avoir un résultats concluant
- [ ] Tester CycleGAN pour transformer des visages humain en Simpsons
- [ ] Prendre en main SDPC
- [ ] Trouver une architecture pour fusionner le GAN et SDPC
- [ ] Evaluer l'intèret de l'architecture
- [ ] Tester BigGan qui promet de bien marcher mais demande beaucoup de ressource : Peut être

## Note d'expériences

#### Test 128 en 128x128 batchsize=32 epochs=400 lrG=0.0004 lrD=0.00004 eps=0.00005
Au vus des résultats de W6_128_dcgan il fallait détailler l'évolution des images avant et pendant les pics des courbes de losses.

__Résultats__ :
  - DCGAN (gt-0 dcgan2): On retrouve les deux même pics autour de 125 et 180 epochs, comme attendue. Les images en ces deux points sont très differentes du reste de l'entrainement. On constate en 132 et 196, juste après le premier et le deuxième pics, des formes très shématique (rectangulair) avec le visage et des yeux. Le réseau semble passer par une phase de simplication des choses aprises jusqu'ici puis un retour à l'apprentissage rapide (140 204) suivie d'un retour au niveau d'avant le pics (152 244) et enfin des images meilleurs en fin d'apprentissage ().  
		Time=12h20
		
__Conclusion__ :
  - Il semble que les pics sont des zones où l'apprentissage est reboot, probablement que le lr est si grand qu'il permet de sortir du minimum où on ce trouve pour tomber dans un autre creux. A voir après les résultats W7_128lr_dcgan qui visent à tester cette hypothèse. 

![W6_DCGAN 128](W7_128_dcgan/176.png "DCGAN 176 Before picks")
![W6_DCGAN 128](W7_128_dcgan/180.png "DCGAN 180 In1 picks")
![W6_DCGAN 128](W7_128_dcgan/184.png "DCGAN 184 In2 picks")
![W6_DCGAN 128](W7_128_dcgan/196.png "DCGAN 196 In3 picks")
![W6_DCGAN 128](W7_128_dcgan/244.png "DCGAN 244 After picks")

![W6_DCGAN 128](W6_128_dcgan/pics.gif "DCGAN 128 Pics Gif 116-224")



#### Test 128lr en 128x128 batchsize=32 epochs=1000 lrG=0.0001 lrD=0.00001 eps=0.00005
Lr plus faible pour voir l'effet sur les pics de l'expérience W6_128_dcgan
Hypothèse : les pics de l'expériences W6_128_dcgan sont du à un lr trop élever.

__Résultats__ :
  - DCGAN :
		Time=
		
__Conclusion__ :
  - :

#### Test 128BN en 128x128 batchsize=32 epochs=200 lrG=0.0004 lrD=0.00004 eps=0.1
Une batchNormalization plus élever (0.00005 -> 0.1).
Hypothèse : La batchNormalization c'est le bien.

__Résultats__ :
  - DCGAN (gt-2 dcgan):
		Time=
		
__Conclusion__ :
  - :

#### Test 128Treshold en 128x128 batchsize=32 epochs=200 lrG=0.0004 lrD=0.00004 eps=0.00005
Modification du losses de G.
Les images générer par G qui sont les mieux discriminer par D (D(G(z)) == 0) sont exclue du losses (objectif = 0.5).

__Résultats__ :
  - DCGAN (gt-0 dcgan2):
		Time=
		
__Conclusion__ :
  - :
